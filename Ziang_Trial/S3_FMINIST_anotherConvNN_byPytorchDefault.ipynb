{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"S3_FMINIST_anotherConvNN_byPytorchDefault.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"cells":[{"cell_type":"code","metadata":{"id":"bjoZY0eX7e2t"},"source":["from __future__ import print_function\n","import argparse\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.optim.lr_scheduler import StepLR\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tjmZSCZFXn1_","outputId":"d412a195-8dcf-4550-ebfb-b05f449f7f7f"},"source":["print(torch.__version__)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1.10.0+cu111\n"]}]},{"cell_type":"markdown","metadata":{"id":"ZQKjQICDpXI_"},"source":["### Download the dataset before."]},{"cell_type":"code","metadata":{"id":"2P6fJPM8Is11"},"source":["\n","# def load_data_subset(batch_size,\n","#                      workers,\n","#                      dataset,\n","#                      data_target_dir,\n","#                      labels_per_class=100,\n","#                      valid_labels_per_class=500,\n","#                      mixup_alpha=1):\n","#     '''return datalaoder'''\n","#     ## copied from GibbsNet_pytorch/load.py\n","#     if dataset == 'cifar10':\n","#         mean = [x / 255 for x in [125.3, 123.0, 113.9]]\n","#         std = [x / 255 for x in [63.0, 62.1, 66.7]]\n","#     elif dataset == 'cifar100':\n","#         mean = [x / 255 for x in [129.3, 124.1, 112.4]]\n","#         std = [x / 255 for x in [68.2, 65.4, 70.4]]\n","#     elif dataset == 'tiny-imagenet-200':\n","#         mean = [x / 255 for x in [127.5, 127.5, 127.5]]\n","#         std = [x / 255 for x in [127.5, 127.5, 127.5]]\n","#     else:\n","#         assert False, \"Unknow dataset : {}\".format(dataset)\n","\n","#     # pre-processing\n","#     if dataset == 'tiny-imagenet-200':\n","#         train_transform = transforms.Compose([\n","#             transforms.RandomHorizontalFlip(),\n","#             transforms.RandomCrop(64, padding=4),\n","#             transforms.ToTensor(),\n","#             transforms.Normalize(mean, std)\n","#         ])\n","\n","#         test_transform = transforms.Compose(\n","#             [transforms.ToTensor(), transforms.Normalize(mean, std)])\n","#     else:\n","#         train_transform = transforms.Compose([\n","#             transforms.RandomHorizontalFlip(),\n","#             transforms.RandomCrop(32, padding=2),\n","#             transforms.ToTensor(),\n","#             transforms.Normalize(mean, std)\n","#         ])\n","\n","#         test_transform = transforms.Compose(\n","#             [transforms.ToTensor(), transforms.Normalize(mean, std)])\n","\n","#     # dataset\n","#     if dataset == 'cifar10':\n","#         train_data = datasets.CIFAR10(data_target_dir,\n","#                                       train=True,\n","#                                       transform=train_transform,\n","#                                       download=True)\n","#         test_data = datasets.CIFAR10(data_target_dir,\n","#                                      train=False,\n","#                                      transform=test_transform,\n","#                                      download=True)\n","#         num_classes = 10\n","#     elif dataset == 'cifar100':\n","#         train_data = datasets.CIFAR100(data_target_dir,\n","#                                        train=True,\n","#                                        transform=train_transform,\n","#                                        download=True)\n","#         test_data = datasets.CIFAR100(data_target_dir,\n","#                                       train=False,\n","#                                       transform=test_transform,\n","#                                       download=True)\n","#         num_classes = 100\n","#     elif dataset == 'tiny-imagenet-200':\n","#         train_root = os.path.join(data_target_dir,\n","#                                   'train')  # this is path to training images folder\n","#         validation_root = os.path.join(data_target_dir,\n","#                                        'val/images')  # this is path to validation images folder\n","#         train_data = datasets.ImageFolder(train_root, transform=train_transform)\n","#         test_data = datasets.ImageFolder(validation_root, transform=test_transform)\n","#         num_classes = 200\n","#     else:\n","#         assert False, 'Do not support dataset : {}'.format(dataset)\n","\n","#     n_labels = num_classes\n","\n","#     # random sampler\n","#     def get_sampler(labels, n=None, n_valid=None):\n","#         # Only choose digits in n_labels\n","#         # n = number of labels per class for training\n","#         # n_val = number of lables per class for validation\n","#         (indices, ) = np.where(reduce(__or__, [labels == i for i in np.arange(n_labels)]))\n","#         np.random.shuffle(indices)\n","\n","#         indices_valid = np.hstack([\n","#             list(filter(lambda idx: labels[idx] == i, indices))[:n_valid] for i in range(n_labels)\n","#         ])\n","#         indices_train = np.hstack([\n","#             list(filter(lambda idx: labels[idx] == i, indices))[n_valid:n_valid + n]\n","#             for i in range(n_labels)\n","#         ])\n","#         indices_unlabelled = np.hstack(\n","#             [list(filter(lambda idx: labels[idx] == i, indices))[:] for i in range(n_labels)])\n","\n","#         indices_train = torch.from_numpy(indices_train)\n","#         indices_valid = torch.from_numpy(indices_valid)\n","#         indices_unlabelled = torch.from_numpy(indices_unlabelled)\n","#         sampler_train = SubsetRandomSampler(indices_train)\n","#         sampler_valid = SubsetRandomSampler(indices_valid)\n","#         sampler_unlabelled = SubsetRandomSampler(indices_unlabelled)\n","#         return sampler_train, sampler_valid, sampler_unlabelled\n","\n","#     if dataset == 'tiny-imagenet-200':\n","#         pass\n","#     else:\n","#         train_sampler, valid_sampler, unlabelled_sampler = get_sampler(\n","#             train_data.targets, labels_per_class, valid_labels_per_class)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l9KhjqxlFsyB"},"source":["# from torch.utils.data.sampler import SubsetRandomSampler\n","# # random sampler\n","# def get_sampler(labels, n=None, n_valid=None):\n","#     # Only choose digits in n_labels\n","#     # n = number of labels per class for training\n","#     # n_val = number of lables per class for validation\n","#     (indices, ) = np.where(reduce(__or__, [labels == i for i in np.arange(n_labels)]))\n","#     np.random.shuffle(indices)\n","\n","#     indices_valid = np.hstack([\n","#         list(filter(lambda idx: labels[idx] == i, indices))[:n_valid] for i in range(n_labels)\n","#     ])\n","#     indices_train = np.hstack([\n","#         list(filter(lambda idx: labels[idx] == i, indices))[n_valid:n_valid + n]\n","#         for i in range(n_labels)\n","#     ])\n","#     indices_unlabelled = np.hstack(\n","#         [list(filter(lambda idx: labels[idx] == i, indices))[:] for i in range(n_labels)])\n","\n","#     indices_train = torch.from_numpy(indices_train)\n","#     indices_valid = torch.from_numpy(indices_valid)\n","#     indices_unlabelled = torch.from_numpy(indices_unlabelled)\n","#     sampler_train = SubsetRandomSampler(indices_train)\n","#     sampler_valid = SubsetRandomSampler(indices_valid)\n","#     sampler_unlabelled = SubsetRandomSampler(indices_unlabelled)\n","#     return sampler_train, sampler_valid, sampler_unlabelled"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O_SzIOqYJIM5"},"source":["# labels_per_class = 10\n","# valid_labels_per_class = 10\n","# sample_size\n","# train_sampler, valid_sampler, unlabelled_sampler = get_sampler(\n","#             train_data.targets, labels_per_class, valid_labels_per_class)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jYruD2KdpJ4A"},"source":["# transform=transforms.Compose([\n","#     transforms.ToTensor(),\n","#     transforms.Normalize((0.1307,), (0.3081,))\n","#     ])\n","# dataset1 = datasets.FashionMNIST('./data', train=True, download=True,\n","#                       transform=transform)\n","# dataset2 = datasets.FashionMNIST('./data', train=False,\n","#                       transform=transform)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hJ-_VHgHM8Wv"},"source":["# import torchvision\n","# import torch\n","\n","# ## FashionMNIST\n","# # trainset = torchvision.datasets.FashionMNIST(root='./data', train=True,\n","# #                                         download=True, transform=None)\n","\n","# ## CIFAR10\n","# trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","#                                         download=True, transform=None)\n","\n","# evens = list(range(0, len(trainset), 2))\n","# odds = list(range(1, len(trainset), 2))\n","# trainset_1 = torch.utils.data.Subset(trainset, evens)\n","# trainset_2 = torch.utils.data.Subset(trainset, odds)\n","\n","# trainloader_1 = torch.utils.data.DataLoader(trainset_1, batch_size=4,\n","#                                             shuffle=True, num_workers=2)\n","# trainloader_2 = torch.utils.data.DataLoader(trainset_2, batch_size=4,\n","#                                             shuffle=True, num_workers=2)\n","\n","\n","# # dataset1 = datasets.CIFAR10('./data', train=True, download=True,\n","# #                       transform=transform)\n","# # dataset2 = datasets.CIFAR10('./data', train=False,\n","# #                       transform=transform)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8ijy3yQpRk9C"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"inWTf2nHLYUa"},"source":["# dataset2.valid_labels_per_class"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xjiPDNLNrF-e"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZSq0ZUXgrGGy","outputId":"bdf7ca0e-9a40-4a18-9457-bdfe4e6f2a7d"},"source":["import torchvision\n","import torch\n","transform = transforms.Compose(\n","    [transforms.ToTensor(),\n","     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","batch_size = 4\n","\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                        download=True, transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n","                                          shuffle=True, num_workers=2)\n","\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                       download=True, transform=transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n","                                         shuffle=False, num_workers=2)\n","\n","classes = ('plane', 'car', 'bird', 'cat',\n","           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"]}]},{"cell_type":"code","metadata":{"id":"Ivfl2PtkXPEY"},"source":["evens = list(range(0, len(trainset), 2))\n","odds = list(range(1, len(trainset), 2))\n","trainset_1 = torch.utils.data.Subset(trainset, evens)\n","trainset_2 = torch.utils.data.Subset(trainset, odds)\n","\n","trainloader_1 = torch.utils.data.DataLoader(trainset_1, batch_size=4,\n","                                            shuffle=True, num_workers=2)\n","trainloader_2 = torch.utils.data.DataLoader(trainset_2, batch_size=4,\n","                                            shuffle=True, num_workers=2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g6qNSIoQoikh"},"source":["quarter = list(range(0, len(trainset), 4))\n","trainset_quarter = torch.utils.data.Subset(trainset, quarter)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fl5gLQWBXRZl"},"source":["dataset1 = trainset\n","dataset2 = testset"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R-wCJ5Wjzavp"},"source":["### Prepare Toy Convolution Net Architecture"]},{"cell_type":"code","metadata":{"id":"myRizeCk7c2H"},"source":["class Net(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(3, 6, 5)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.conv2 = nn.Conv2d(6, 16, 5)\n","        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n","        self.fc2 = nn.Linear(120, 84)\n","        self.fc3 = nn.Linear(84, 10)\n","\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.conv1(x)))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = torch.flatten(x, 1) # flatten all dimensions except batch\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6R1iK3sh7lm_"},"source":["def train(args, model, device, train_loader, optimizer, epoch):\n","    model.train()\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        data, target = data.to(device), target.to(device)\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss_func = nn.CrossEntropyLoss()\n","        loss = loss_func(output, target)\n","        loss.backward()\n","        optimizer.step()\n","        # if batch_idx % args.log_interval == 0:\n","        #     print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","        #         epoch, batch_idx * len(data), len(train_loader.dataset),\n","        #         100. * batch_idx / len(train_loader), loss.item()))\n","        #     if args.dry_run:\n","        #         break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yZnTwxmB7oFY"},"source":["def test(args, model, device, test_loader, epoch):\n","    model.eval()\n","\n","    test_loss = 0\n","    correct = 0\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(device), target.to(device)\n","            output = model(data)\n","            loss_func = nn.CrossEntropyLoss()\n","            test_loss += loss_func(output, target).item()  # sum up batch loss\n","            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    test_loss /= len(test_loader.dataset)\n","\n","    # print('\\nTest set: Epoch: {} Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","    #     epoch, test_loss, correct, len(test_loader.dataset),\n","    #     (100. * round(correct / len(test_loader.dataset), 6)))\n","    \n","    args.accuracy_list.append(100. * round(correct / len(test_loader.dataset), 6))\n","    args.loss_list.append(test_loss)\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7DdIyvIazkyX"},"source":["### Declare simulated Args Class [contain learning rate and momentum coeff]"]},{"cell_type":"code","metadata":{"id":"gTJKiZ2lhWWh"},"source":["# round(1234/7, 10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qHwEddJ_AEBq"},"source":["class SimulatedArgs():\n","    def __init__(self, epochs, batch_size, optimizer, dry_run, lr_AdaDelta=1, lr_SGD=0.1, momentum=0.9, lr_Adam = 0.01):\n","        # Training settings\n","        self.epochs = epochs\n","\n","        self.batch_size = batch_size  # or 128 [later]\n","        self.test_batch_size = 1000\n","        self.no_cuda = False\n","        \n","        self.optimizer = optimizer\n","          # for AdaDelta optimizer\n","        if (self.optimizer == \"AdaDelta\"):\n","          self.lr = lr_AdaDelta\n","        elif (self.optimizer == \"SGD\"):\n","          # for SGD optimizer\n","          self.lr_SGD = lr_SGD\n","          self.momentum_SGD = 0.9\n","        elif (self.optimizer == \"Adam\"):\n","          self.lr_Adam = lr_Adam\n","\n","        self.gamma = 0.7\n","        self.seed = 1\n","\n","        self.log_interval = 10\n","        self.dry_run = dry_run\n","        self.save_model = False\n","\n","        # Lists for visualization of loss and accuracy \n","        self.loss_list = []\n","        self.iteration_list = []\n","        self.accuracy_list = []\n","\n","        # Lists for knowing classwise accuracy\n","        self.predictions_list = []\n","        self.labels_list = []"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vvDv2ZeE7rJF"},"source":["def main(args, dataset1, dataset2):\n","  use_cuda = not args.no_cuda and torch.cuda.is_available()\n","\n","  torch.manual_seed(args.seed)\n","\n","  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","  train_kwargs = {'batch_size': args.batch_size}\n","  test_kwargs = {'batch_size': args.test_batch_size}\n","  train_kwargs = {'num_workers': 2}\n","  test_kwargs = {'num_workers': 2}\n","  if use_cuda:\n","      cuda_kwargs = {'num_workers': 1,\n","                      'pin_memory': True,\n","                      'shuffle': True}\n","      train_kwargs.update(cuda_kwargs)\n","      test_kwargs.update(cuda_kwargs)\n","\n","  train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n","  test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n","\n","  model = Net().to(device)\n","\n","  ## ---------------------------- IMPORTANT ----------------------------------\n","  # choose correct optimizer\n","  print('\\nTraining SetUp: batch size: {}, optimizer: {} \\n'.format(args.batch_size, args.optimizer))\n","  if (args.optimizer == \"AdaDelta\"):\n","    print('Inner Parameters: lr: {} \\n'.format(args.lr))\n","    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n","\n","  elif (args.optimizer == \"SGD\"):\n","    print('Inner Parameters: lr_SGD: {}, momentum_SGD: {} \\n'.format(args.lr_SGD, args.momentum_SGD))\n","    optimizer = optim.SGD(model.parameters(), lr=args.lr_SGD, momentum=args.momentum_SGD)\n","  elif (args.optimizer == \"Adam\"):\n","    print('Inner Parameters: lr_Adam: {} \\n'.format(args.lr_Adam))\n","    optimizer = optim.Adam(model.parameters(), lr=args.lr_Adam)\n","\n","\n","  # set linear rate updater\n","  scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n","\n","  ## ---------------------------- IMPORTANT ----------------------------------\n","  \n","  for epoch in range(1, args.epochs + 1):\n","      args.iteration_list.append(epoch)\n","      train(args, model, device, train_loader, optimizer, epoch)\n","      test(args, model, device, test_loader, epoch)\n","      scheduler.step()  # To update the learning rates\n","  \n","      if (epoch == args.epochs):\n","          print('\\nTest set: Epoch: {} Average loss: {:.4f}, Accuracy: ({:.4f}%)\\n'.format(\n","          epoch, args.iteration_list[-1], args.accuracy_list[-1]))\n","      \n","  plt.plot(args.iteration_list, args.loss_list)\n","  plt.xlabel(\"No. of Iteration\")\n","  plt.ylabel(\"Loss\")\n","  plt.title(\"Iterations vs Loss\")\n","  plt.show()\n","\n","  plt.plot(args.iteration_list, args.accuracy_list)\n","  plt.xlabel(\"No. of Iteration\")\n","  plt.ylabel(\"Accuracy\")\n","  plt.title(\"Iterations vs Accuracy\")\n","  plt.show()\n","\n","  if args.save_model:\n","      torch.save(model.state_dict(), str(len(dataset1)) + \"cifar10_cnn.pt\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-bk7BZ_UzSf9"},"source":["# Start Testing!!!"]},{"cell_type":"markdown","metadata":{"id":"0SZ1pDvFi6d6"},"source":["## When the size of training data is relativly close, the accuracy do it as well."]},{"cell_type":"code","metadata":{"id":"AqUM6uTFrNLb"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6_yb1nAapmAc"},"source":["args = SimulatedArgs(epochs=50, batch_size=128, optimizer = \"Adam\", dry_run = False)\n","main(args, dataset1, dataset2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"Kq9XKraerNS8"},"source":["# [1] Training SetUp: batch size: 128, optimizer: SGD \n","args = SimulatedArgs(epochs=50, batch_size=128, optimizer = \"Adam\", dry_run = False)\n","main(args, trainset_1, dataset2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DnNzrxsFpjDT"},"source":["args = SimulatedArgs(epochs=50, batch_size=128, optimizer = \"Adam\", dry_run = False)\n","main(args, trainset_2, dataset2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1sEfrJJmZZoo"},"source":["args = SimulatedArgs(epochs=50, batch_size=128, optimizer = \"Adam\", dry_run = False)\n","main(args, trainset_quarter, dataset2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i8SfkHgWjP3h"},"source":["## When we use full data set --- the improvement of accuracy is obvious"]}]}